- ğŸ‘‹ Hi, Iâ€™m @taldrete
- ğŸ‘€ Iâ€™m interested in Data Analytics, everything related to Tech, and learning how things work. Outside of tech, I love football Go Arsenal!
- ğŸŒ± Iâ€™m currently learning Machine Learning
- ğŸ’ï¸ Iâ€™m looking to collaborate on testing projects and helping others
- ğŸ“« How to reach me email: taldrete@pm.me
- ğŸ˜„ Pronouns: Him
- âš¡ Fun fact: I once owned a company/business called "Prestige Worldwide"

<!---
taldrete/taldrete is a âœ¨ special âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
I want to share the capstone project I had to complete to get the Google Data Analytics certification. I share this in the hope it can help others dealing with the same or similar project.

The Project is a Case Study of a fictitious company called BellaBeat. Bellabeat is a high-tech company specializing in wearables to track and measure female health data.
I was tasked with finding the answer to 3 questions for the stakeholders

1. What are the trends in smart device usage?
2. How could this trend apply to BellaBeat customers?
3. How could these trends help influence BellaBeat's marketing strategy?

To answer these questions the stakeholder encouraged me to use data from a competitor. The data used was
**FitBit Fitness Tracker Data** *(CC0: Public Domain, dataset made
available through Mobius): This Kaggle data set contains personal
fitness tracker from thirty fitbit users. Thirty eligible Fitbit users
consented to the submission of personal tracker data, including
minute-level output for physical activity, heart rate, and sleep
monitoring. It includes information about daily activity, steps, and
heart rate that can be used to explore usersâ€™ habits.)*

With a clear and defined business task at hand, I got to work. After getting the data set and glancing over the dataset it was a lot of data to explore, clean, and prepare. The files were CSV.
I started opening the files in Excel and as I was going through I got frustrated and decided that it was not the right tool for the job. I debated if I should use SQL or R. I decided to go with SQL.
I used Big Query to create the Database and started uploading the tables everything was great until I had issues with Big Query and got stuck. I looked for a solution to the issue but realized that
it was more time spent troubleshooting than working on the project. So I decided to go with R Studio. I will upload a markdown file to share the process, code, and results of the project. I ended up 
starting from scratch and while there was frustration at times I found the experience to be educational. I know others will go through the same experience and I hope that my project serves as encouragement
to not give up and continue. My biggest challenge was translating my thoughts into the R code I used the systematic approach and found it simpler to work with one table at a time and at the end
consolidate the results in a notebook.
